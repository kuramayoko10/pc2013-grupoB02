A indústria de computadores, desde seu começo, sempre esteve em uma jornada para aumentar o poder de computação. De 1947 até 2013, o número de instruções por segundo passou da ordem de 10² para 10⁹[1]. No entanto, o aumento exponencial do poder de processamento de um único chip não se manterá por muito tempo, em breve barreiras como o tamanho mínimo de um transistor, velocidade máxima dos eletrons em um semicondutor e a dissipação de calor serão encontradas.
Através da utilização de processadores paralelos, podemos aumentar a velocidade com que problemas são resolvidos. Através de muitos processadores trabalhando em velocidades normais e em conjunto realizando tarefas.
Dentre as possíveis abordagens para se aplicar o parelelismo, temos os ambientes de memória compartilhada: Um sistema com diversas CPUs que operam em um mesmo espaço de memória, os multiprocessadores de hoje em dia normalmente operam dessa forma. No entanto, essa abordagem não é escalável para um número de processadores muito grande: O acesso a memória acaba se tornando um gargalo. Além de que manter a coerência entre os caches se torna um processo muito custoso. Para se aplicar essa abordagem, normalmente se utiliza as bibliotecas pthreads ou o padrão OpenMP.
Outra abordagem é a utilização de diversos processadores nos quais operam processos autônomos, que comunicam-se uns com os outros por meio da troca de mensagens. Essas mensagens podem ser enviadas por meio de uma conexão de hardware própria ou por uma rede LAN.

